{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = docx2txt.process('/home/iba/Downloads/Content.docx')\n",
    "#raw_html = raw_html.read()\n",
    "\n",
    "article_html = bs.BeautifulSoup(rnn, 'lxml')\n",
    "\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:\n",
    "    article_text += para.text\n",
    "\n",
    "article_text = article_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'iris modelling using feed forward neural network\\n\\n\\n\\nfisher\\u2019s iris database (fisher, 1936) is perhaps the best known database to be found in the pattern recognition literature. the data set contains 3 classes of 50 instances(samples) each, where each class refers to a type of iris plant. one class is linearly separable from the other two; the latter are not linearly separable from each other. the database contains the following attributes: \\n\\n\\n\\n1). sepal length in cm \\n\\n2). sepal width in cm \\n\\n3). petal length in cm \\n\\n4). petal width in cm \\n\\n5). class: \\n\\n- iris setosa \\n\\n- iris versicolour\\n\\n - iris virginica \\n\\na feedforward neural network is a biologically inspired classification algorithm. it consists of a (possibly large) number of simple neuron-like processing units, organized in layers. every unit in a layer is connected with all the units in the previous layer. these connections are not all equal: each connection may have a different strength or weight. the weights on these connections encode the knowledge of a network. often the units in a neural network are also called nodes.\\n\\ndata enters at the inputs and passes through the network, layer by layer, until it arrives at the outputs. during normal operation, that is when it acts as a classifier, there is no feedback between layers. this is why they are called feedforward neural networks.\\n\\nin the following figure we see an example of a 2-layered network with, from top to bottom: an output layer with 5 units, a hidden layer with 4 units, respectively. the network has 3 input units.\\n\\n\\n\\nthe 3 inputs are shown as circles and these do not belong to any layer of the network (although the inputs sometimes are considered as a virtual layer with layer number 0). any layer that is not an output layer is a hidden layer. this network therefore has 1 hidden layer and 1 output layer. the figure also shows all the connections between the units in different layers. a layer only connects to the previous layer.\\n\\nfeedforward networks often have one or more hidden layers of sigmoid neurons followed by an output layer of linear neurons. multiple layers of neurons with nonlinear transfer functions allow the network to learn nonlinear and linear relationships between input and output vectors. the linear output layer lets the network produce values outside the range -1 to +1. \\n\\n\\n\\nhow to load iris.csv dataset\\n\\n\\n\\nload csv files to python pandas\\n\\nload the pandas libraries with alias \\'pd\\'\\n\\nimport pandas as pd.\\n\\nread data from file \\'filename.csv\\'\\n\\n(in the same directory that your python process is based)\\n\\ncontrol delimiters, rows, column names with read_csv \\n\\ndata = pd.read_csv ( url or path  of iris.csv )\\n\\nthe following code is to download iris dataset\\n\\n\\n\\nimport pandas as pd\\ndata = pd.read_csv(\\'iris/iris.csv\\')\\n\\nprint(data)\\n\\n\\t \\t \\t\\n\\n\\n\\n\\n\\nsplit the iris dataset into train set and test set\\n\\n\\n\\ntrain_test_split is a function in sklearn model selection for splitting data arrays into two subsets: for training data and for testing data. with this function, you don\\'t need to divide the dataset manually. by default, sklearn train_test_split will make random partitions for the two subsets.\\n\\n\\n\\nwe are going to separate the data. one part will be used to make predictions in the end, the other part, the most important will be used for training and testing the neural network. \\n\\n\\n\\ndata = data.drop([\\'id\\'], axis =1)\\nx = data.drop([\\'species\\'], axis = 1)\\ny = data[\\'species\\']\\n\\n\\t \\t \\t\\n\\n\\n\\n\\n\\nwe must transform the column of classes, because we have a format \\'str\\', and it is a multiclass situation. we must first convert the names of species into numerical values, then into vectors for the output of the neuron network.\\n\\n\\n\\ny=pd.get_dummies(y)\\nimport sklearn\\n\\nfrom sklearn import preprocessing, model_selection\\n\\n\\ntrain_x, test_x, train_y, test_y = model_selection.train_test_split(x,y,test_size = 0.1, random_state = 0)\\n\\n\\t \\t \\t\\n\\n\\n\\n\\n\\ntest_size \\u2014 this parameter decides the size of the data that has to be split as the test dataset. ...\\n\\ntrain_size \\u2014 you have to specify this parameter only if you\\'re not specifying the test_size. ...\\n\\nrandom_state \\u2014 here you pass an integer, which will act as the seed for the random number generator during the split.\\n\\n\\n\\ncreating a sequential model using keras\\n\\nthe simplest model is defined in the sequential class which is a linear stack of layers. you can create a sequential model and define all of the layers in the constructor, for example:\\n\\n\\n\\nfrom keras.models import sequential \\nmodel = sequential(...)\\n\\nfrom keras the model is building\\n\\n(or)\\n\\nimport tensorflow as tf\\nmodel = tf.keras.sequential()\\n\\nfrom tensorflow the model is building\\n\\nyou can choose any one from these. but here we are choosing tensorflow \\n\\nthe sequential api allows you to create models layer-by-layer for most problems. it is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs.\\n\\nthe sequential model api is a way of creating deep learning models where an instance of the sequential class is created and model layers are created and added to it.\\n\\n\\n\\nimport tensorflow as tf\\n\\n\\n\\ninput_dim = len(data.columns) - 1\\n\\nprint(input_dim)\\n\\nmodel = tf.keras.sequential()\\n\\noutput:\\n\\n4\\n\\n\\t \\t \\t\\n\\n\\n\\n\\n\\nthe model type that we will be using is sequential. sequential is the easiest way to build a model in keras(or) tensorflow . it allows you to build a model layer by layer. each layer has weights that correspond to the layer that follows it.\\n\\n\\n\\nadding number of layers\\n\\n\\n\\na dense layer is just a regular layer of neurons in a neural network. each neuron receives input from all the neurons in the previous layer, thus densely connected. the layer has a weight matrix w, a bias vector b, and the activations of previous layer a.\\n\\n\\n\\nmodel.add(tf.keras.layers.dense(8, input_dim = input_dim , activation = \\'relu\\'))\\n\\nmodel.add(tf.keras.layers.dense(10, activation = \\'relu\\'))\\n\\nmodel.add(tf.keras.layers.dense(10, activation = \\'relu\\'))\\n\\nmodel.add(tf.keras.layers.dense(10, activation = \\'relu\\'))\\n\\nmodel.add(tf.keras.layers.dense(3, activation = \\'softmax\\'))\\n\\nmodel.summary()\\n\\n\\n\\n\\n\\n\\t \\t \\t\\n\\n\\n\\noutput:\\n\\nmodel: \"sequential\"\\n\\n_________________________________________________________________\\n\\nlayer (type)                 output shape              param #   \\n\\n=================================================================\\n\\ndense (dense)                (none, 8)                 40        \\n\\n_________________________________________________________________\\n\\ndense_1 (dense)              (none, 10)                90        \\n\\n_________________________________________________________________\\n\\ndense_2 (dense)              (none, 10)                110       \\n\\n_________________________________________________________________\\n\\ndense_3 (dense)              (none, 10)                110       \\n\\n_________________________________________________________________\\n\\ndense_4 (dense)              (none, 3)                 33        \\n\\n=================================================================\\n\\ntotal params: 383\\n\\ntrainable params: 383\\n\\nnon-trainable params: 0\\n\\n_________________________________________________________________\\n\\n\\n\\n\\n\\nnotice how the first dense object specified in the array is not the input layer. the first dense object is the first hidden layer. the input layer is specified as a parameter to the first dense object\\u2019s constructor.\\n\\nthis is why our input dimensions are specified as input_dim= input_dim . our first hidden layer has eight nodes  and the second hidden layer has 10 and does upto four layers, and the fifth layer has three nodes.\\n\\nfor now, just note that we are using an activation function called relu activation=\\'relu\\' for both of our hidden layers and an activation function called softmax activation=\\'softmax\\' for our output layer. \\n\\nwe use the \\u2018add()\\u2019 function to add layers to our model. \\n\\n\\u2018dense\\u2019 is the layer type. dense is a standard layer type that works for most cases. in a dense layer, all nodes in the previous layer connect to the nodes in the current layer.\\n\\n\\n\\ncompiling the model with respective loss functions and optimizers\\n\\n\\n\\nit is time to make predictions with the small sample removed from the base at the beginning. to train the neural network it was necessary to convert the species into vectors. so after the prediction it is necessary to carry out the opposite operation to recover the name of the associated species.\\n\\nneural networks are trained using an optimization process that requires a loss function to calculate the model error. cross-entropy and mean squared error are the two main types of loss functions to use when training neural network models.\\n\\nthe purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\\n\\n\\n\\nmodel.compile(\\n\\n    loss=tf.keras.losses.binarycrossentropy(from_logits=true),\\n\\n    optimizer=\\'adam\\',\\n\\n    metrics=[\\'accuracy\\'])\\n\\n\\t \\t \\t\\n\\n\\n\\n\\n\\nloss is nothing but a prediction error of neural net. and the method to calculate the loss is called loss function. in simple words, the loss is used to calculate the gradients. and gradients are used to update the weights of the neural net.\\n\\n\\n\\noptimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. optimizers help to get results faster.\\n\\n\\n\\noptimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses.  optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\\n\\nadam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iteratively based on training data. the algorithm is called adam. it is not an acronym and is not written as \\u201cadam\\u201d. the name adam is derived from adaptive moment estimation.\\n\\n\\n\\na metric is a function that is used to judge the performance of your model. metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. note that you may use any loss functions as a metric function.\\n\\n\\n\\ntraining the model\\n\\n\\n\\nnow we will train our model. to train, we will use the \\u2018fit()\\u2019 function on our model with the following four parameters: training data (train_x), target data (train_y), the number of epochs and batch_size.\\n\\n\\n\\nmodel.fit(train_x, train_y, epochs = 10, batch_size = 2)\\n\\n\\t \\t \\t\\n\\n\\n\\noutput:\\n\\ntrain on 135 samples\\n\\nepoch 1/10\\n\\n135/135 [==============================] - 0s 1ms/sample - loss: 0.5854 - accuracy: 0.9605\\n\\nepoch 2/10\\n\\n135/135 [==============================] - 0s 1ms/sample - loss: 0.5814 - accuracy: 0.9802\\n\\nepoch 3/10\\n\\n135/135 [==============================] - 0s 1ms/sample - loss: 0.5819 - accuracy: 0.9753\\n\\nepoch 4/10\\n\\n135/135 [==============================] - 0s 1ms/sample - loss: 0.5796 - accuracy: 0.9802\\n\\nepoch 5/10\\n\\n135/135 [==============================] - 0s 2ms/sample - loss: 0.5798 - accuracy: 0.9753\\n\\nepoch 6/10\\n\\n135/135 [==============================] - 0s 2ms/sample - loss: 0.5808 - accuracy: 0.9802\\n\\nepoch 7/10\\n\\n135/135 [==============================] - 0s 1ms/sample - loss: 0.5820 - accuracy: 0.9753\\n\\nepoch 8/10\\n\\n135/135 [==============================] - 0s 2ms/sample - loss: 0.5790 - accuracy: 0.9802\\n\\nepoch 9/10\\n\\n135/135 [==============================] - 0s 1ms/sample - loss: 0.5805 - accuracy: 0.9753\\n\\nepoch 10/10\\n\\n135/135 [==============================] - 0s 2ms/sample - loss: 0.5820 - accuracy: 0.9704\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nthe number of epochs is the number of times the model will cycle through the data. the more epochs we run, the more the model will improve, up to a certain point.\\n\\n\\n\\nyou should set the number of epochs as high as possible and terminate training based on the error rates. just to be clear, an epoch is one learning cycle where the learner sees the whole training data set. if you have two batches, the learner needs to go through two iterations for one epoch.\\n\\nthe batch size is a number of samples processed before the model is updated. the number of epochs is the number of complete passes through the training dataset. the size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\\n\\n\\n\\nin the neural network terminology: one epoch = one forward pass and one backward pass of all the training examples. batch size = the number of training examples in one forward/backward pass. the higher the batch size, the more memory space you\\'ll need\\n\\n\\n\\n\\n\\nevaluating the models accuracy \\n\\n\\n\\nmodel evaluation is an integral part of the model development process. it helps to find the best model that represents our data and how well the chosen model will work in the future.  to avoid overfitting, both methods use a test set (not seen by the model) to evaluate model performance.\\n\\n\\n\\nscores = model.evaluate(test_x, test_y)\\nprint(\"test score\",scores[0])\\nprint(\"test accuracy\",scores[1])\\n\\n\\t \\t \\t\\n\\n\\n\\noutput:\\n\\n15/15 [==============================] - 0s 174us/sample - loss: 0.5799 - accuracy: 0.9556\\n\\n(\\'test score\\', 0.5798901319503784)\\n\\n(\\'test accuracy\\', 0.95555556)\\n\\n\\n\\nthe model. evaluate function predicts the output for the given input and then computes the metrics function specified in the model. so even if you use the same data, the differences will be there because the value of a loss function will be almost always different than the predicted values.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "article_text = re.sub(r'\\s+', ' ', article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'iris modelling using feed forward neural network fisher\\u2019s iris database (fisher, 1936) is perhaps the best known database to be found in the pattern recognition literature. the data set contains 3 classes of 50 instances(samples) each, where each class refers to a type of iris plant. one class is linearly separable from the other two; the latter are not linearly separable from each other. the database contains the following attributes: 1). sepal length in cm 2). sepal width in cm 3). petal length in cm 4). petal width in cm 5). class: - iris setosa - iris versicolour - iris virginica a feedforward neural network is a biologically inspired classification algorithm. it consists of a (possibly large) number of simple neuron-like processing units, organized in layers. every unit in a layer is connected with all the units in the previous layer. these connections are not all equal: each connection may have a different strength or weight. the weights on these connections encode the knowledge of a network. often the units in a neural network are also called nodes. data enters at the inputs and passes through the network, layer by layer, until it arrives at the outputs. during normal operation, that is when it acts as a classifier, there is no feedback between layers. this is why they are called feedforward neural networks. in the following figure we see an example of a 2-layered network with, from top to bottom: an output layer with 5 units, a hidden layer with 4 units, respectively. the network has 3 input units. the 3 inputs are shown as circles and these do not belong to any layer of the network (although the inputs sometimes are considered as a virtual layer with layer number 0). any layer that is not an output layer is a hidden layer. this network therefore has 1 hidden layer and 1 output layer. the figure also shows all the connections between the units in different layers. a layer only connects to the previous layer. feedforward networks often have one or more hidden layers of sigmoid neurons followed by an output layer of linear neurons. multiple layers of neurons with nonlinear transfer functions allow the network to learn nonlinear and linear relationships between input and output vectors. the linear output layer lets the network produce values outside the range -1 to +1. how to load iris.csv dataset load csv files to python pandas load the pandas libraries with alias \\'pd\\' import pandas as pd. read data from file \\'filename.csv\\' (in the same directory that your python process is based) control delimiters, rows, column names with read_csv data = pd.read_csv ( url or path of iris.csv ) the following code is to download iris dataset import pandas as pd data = pd.read_csv(\\'iris/iris.csv\\') print(data) split the iris dataset into train set and test set train_test_split is a function in sklearn model selection for splitting data arrays into two subsets: for training data and for testing data. with this function, you don\\'t need to divide the dataset manually. by default, sklearn train_test_split will make random partitions for the two subsets. we are going to separate the data. one part will be used to make predictions in the end, the other part, the most important will be used for training and testing the neural network. data = data.drop([\\'id\\'], axis =1) x = data.drop([\\'species\\'], axis = 1) y = data[\\'species\\'] we must transform the column of classes, because we have a format \\'str\\', and it is a multiclass situation. we must first convert the names of species into numerical values, then into vectors for the output of the neuron network. y=pd.get_dummies(y) import sklearn from sklearn import preprocessing, model_selection train_x, test_x, train_y, test_y = model_selection.train_test_split(x,y,test_size = 0.1, random_state = 0) test_size \\u2014 this parameter decides the size of the data that has to be split as the test dataset. ... train_size \\u2014 you have to specify this parameter only if you\\'re not specifying the test_size. ... random_state \\u2014 here you pass an integer, which will act as the seed for the random number generator during the split. creating a sequential model using keras the simplest model is defined in the sequential class which is a linear stack of layers. you can create a sequential model and define all of the layers in the constructor, for example: from keras.models import sequential model = sequential(...) from keras the model is building (or) import tensorflow as tf model = tf.keras.sequential() from tensorflow the model is building you can choose any one from these. but here we are choosing tensorflow the sequential api allows you to create models layer-by-layer for most problems. it is limited in that it does not allow you to create models that share layers or have multiple inputs or outputs. the sequential model api is a way of creating deep learning models where an instance of the sequential class is created and model layers are created and added to it. import tensorflow as tf input_dim = len(data.columns) - 1 print(input_dim) model = tf.keras.sequential() output: 4 the model type that we will be using is sequential. sequential is the easiest way to build a model in keras(or) tensorflow . it allows you to build a model layer by layer. each layer has weights that correspond to the layer that follows it. adding number of layers a dense layer is just a regular layer of neurons in a neural network. each neuron receives input from all the neurons in the previous layer, thus densely connected. the layer has a weight matrix w, a bias vector b, and the activations of previous layer a. model.add(tf.keras.layers.dense(8, input_dim = input_dim , activation = \\'relu\\')) model.add(tf.keras.layers.dense(10, activation = \\'relu\\')) model.add(tf.keras.layers.dense(10, activation = \\'relu\\')) model.add(tf.keras.layers.dense(10, activation = \\'relu\\')) model.add(tf.keras.layers.dense(3, activation = \\'softmax\\')) model.summary() output: model: \"sequential\" _________________________________________________________________ layer (type) output shape param # ================================================================= dense (dense) (none, 8) 40 _________________________________________________________________ dense_1 (dense) (none, 10) 90 _________________________________________________________________ dense_2 (dense) (none, 10) 110 _________________________________________________________________ dense_3 (dense) (none, 10) 110 _________________________________________________________________ dense_4 (dense) (none, 3) 33 ================================================================= total params: 383 trainable params: 383 non-trainable params: 0 _________________________________________________________________ notice how the first dense object specified in the array is not the input layer. the first dense object is the first hidden layer. the input layer is specified as a parameter to the first dense object\\u2019s constructor. this is why our input dimensions are specified as input_dim= input_dim . our first hidden layer has eight nodes and the second hidden layer has 10 and does upto four layers, and the fifth layer has three nodes. for now, just note that we are using an activation function called relu activation=\\'relu\\' for both of our hidden layers and an activation function called softmax activation=\\'softmax\\' for our output layer. we use the \\u2018add()\\u2019 function to add layers to our model. \\u2018dense\\u2019 is the layer type. dense is a standard layer type that works for most cases. in a dense layer, all nodes in the previous layer connect to the nodes in the current layer. compiling the model with respective loss functions and optimizers it is time to make predictions with the small sample removed from the base at the beginning. to train the neural network it was necessary to convert the species into vectors. so after the prediction it is necessary to carry out the opposite operation to recover the name of the associated species. neural networks are trained using an optimization process that requires a loss function to calculate the model error. cross-entropy and mean squared error are the two main types of loss functions to use when training neural network models. the purpose of loss functions is to compute the quantity that a model should seek to minimize during training. model.compile( loss=tf.keras.losses.binarycrossentropy(from_logits=true), optimizer=\\'adam\\', metrics=[\\'accuracy\\']) loss is nothing but a prediction error of neural net. and the method to calculate the loss is called loss function. in simple words, the loss is used to calculate the gradients. and gradients are used to update the weights of the neural net. optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. optimizers help to get results faster. optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible. adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iteratively based on training data. the algorithm is called adam. it is not an acronym and is not written as \\u201cadam\\u201d. the name adam is derived from adaptive moment estimation. a metric is a function that is used to judge the performance of your model. metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. note that you may use any loss functions as a metric function. training the model now we will train our model. to train, we will use the \\u2018fit()\\u2019 function on our model with the following four parameters: training data (train_x), target data (train_y), the number of epochs and batch_size. model.fit(train_x, train_y, epochs = 10, batch_size = 2) output: train on 135 samples epoch 1/10 135/135 [==============================] - 0s 1ms/sample - loss: 0.5854 - accuracy: 0.9605 epoch 2/10 135/135 [==============================] - 0s 1ms/sample - loss: 0.5814 - accuracy: 0.9802 epoch 3/10 135/135 [==============================] - 0s 1ms/sample - loss: 0.5819 - accuracy: 0.9753 epoch 4/10 135/135 [==============================] - 0s 1ms/sample - loss: 0.5796 - accuracy: 0.9802 epoch 5/10 135/135 [==============================] - 0s 2ms/sample - loss: 0.5798 - accuracy: 0.9753 epoch 6/10 135/135 [==============================] - 0s 2ms/sample - loss: 0.5808 - accuracy: 0.9802 epoch 7/10 135/135 [==============================] - 0s 1ms/sample - loss: 0.5820 - accuracy: 0.9753 epoch 8/10 135/135 [==============================] - 0s 2ms/sample - loss: 0.5790 - accuracy: 0.9802 epoch 9/10 135/135 [==============================] - 0s 1ms/sample - loss: 0.5805 - accuracy: 0.9753 epoch 10/10 135/135 [==============================] - 0s 2ms/sample - loss: 0.5820 - accuracy: 0.9704 the number of epochs is the number of times the model will cycle through the data. the more epochs we run, the more the model will improve, up to a certain point. you should set the number of epochs as high as possible and terminate training based on the error rates. just to be clear, an epoch is one learning cycle where the learner sees the whole training data set. if you have two batches, the learner needs to go through two iterations for one epoch. the batch size is a number of samples processed before the model is updated. the number of epochs is the number of complete passes through the training dataset. the size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset. in the neural network terminology: one epoch = one forward pass and one backward pass of all the training examples. batch size = the number of training examples in one forward/backward pass. the higher the batch size, the more memory space you\\'ll need evaluating the models accuracy model evaluation is an integral part of the model development process. it helps to find the best model that represents our data and how well the chosen model will work in the future. to avoid overfitting, both methods use a test set (not seen by the model) to evaluate model performance. scores = model.evaluate(test_x, test_y) print(\"test score\",scores ) print(\"test accuracy\",scores ) output: 15/15 [==============================] - 0s 174us/sample - loss: 0.5799 - accuracy: 0.9556 (\\'test score\\', 0.5798901319503784) (\\'test accuracy\\', 0.95555556) the model. evaluate function predicts the output for the given input and then computes the metrics function specified in the model. so even if you use the same data, the differences will be there because the value of a loss function will be almost always different than the predicted values.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_sentences = nltk.sent_tokenize(article_text)\n",
    "article_words = nltk.word_tokenize(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def perform_lemmatization(tokens):\n",
    "    return [wnlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
    "\n",
    "def get_processed_text(document):\n",
    "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = (\"hey\", \"good morning\", \"good evening\", \"morning\", \"evening\", \"hi\", \"whatsup\")\n",
    "greeting_responses = [\"hey\", \"hey hows you?\", \"*nods*\", \"hello, how you doing\", \"hello\", \"Welcome, I am good and you\"]\n",
    "\n",
    "def generate_greeting_response(greeting):\n",
    "    for token in greeting.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input):\n",
    "    robo_response = ''\n",
    "    article_sentences.append(user_input)\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
    "    all_word_vectors = word_vectorizer.fit_transform(article_sentences)\n",
    "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "\n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    vector_matched = matched_vector[-2]\n",
    "\n",
    "    if vector_matched == 0:\n",
    "        robo_response = robo_response + \"I am sorry, I could not understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response + article_sentences[similar_sentence_number]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
    "all_word_vectors = word_vectorizer.fit_transform(article_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_sentence_number = similar_vector_values.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am your friend Ai team. You can ask me any question:\n",
      "'what is model.evaluate function'\n",
      "Ai team:\n",
      "note that you may use any loss functions as a metric function.\n",
      "'loss functions'\n",
      "Ai team:\n",
      "and the method to calculate the loss is called loss function.\n",
      "'iris'\n",
      "Ai team:\n",
      "class: - iris setosa - iris versicolour - iris virginica a feedforward neural network is a biologically inspired classification algorithm.\n",
      "'what is adding number of layers'\n",
      "Ai team:\n",
      "adding number of layers a dense layer is just a regular layer of neurons in a neural network.\n",
      "'bye'\n",
      "Ai team: Good bye and take care of yourself...\n"
     ]
    }
   ],
   "source": [
    "continue_dialogue = True\n",
    "print(\"Hello, I am your friend Ai team. You can ask me any question:\")\n",
    "while(continue_dialogue == True):\n",
    "    human_text = input()\n",
    "    human_text = human_text.lower()\n",
    "    if human_text != 'bye':\n",
    "        if human_text == 'thanks' or human_text == 'thank you very much' or human_text == 'exit':\n",
    "            continue_dialogue = False\n",
    "            print(\"Ai team: Most welcome\")\n",
    "        else:\n",
    "            if generate_greeting_response(human_text) != None:\n",
    "                print(\"Ai team: \" + generate_greeting_response(human_text))\n",
    "            else:\n",
    "                print(\"Ai team:\")\n",
    "                print(generate_response(human_text))\n",
    "                article_sentences.remove(human_text)\n",
    "    else:\n",
    "        continue_dialogue = False\n",
    "        print(\"Ai team: Good bye and take care of yourself...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
